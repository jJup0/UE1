\documentclass[a4paper,%
11pt,%
DIV=12,
headsepline,%
headings=normal,
]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[automark]{scrlayer-scrpage}
\usepackage{graphicx}
\usepackage{lmodern} 
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{listings}

\usepackage{pdfpages}
\usepackage{graphicx}
\graphicspath{ {../plots/} }

\lstset{
  basicstyle=\ttfamily\footnotesize,
  frame=single
}

\newcounter{curex}
\setcounter{curex}{0}
\newcommand{\exercise}[1]{\section*{Exercise #1}\setcounter{curex}{#1}}
\newcommand{\answer}[1]{\subsection*{Answer \arabic{curex}.#1}}

\newcommand\plotwidth{0.8\textwidth}
\newcommand\plotheight{0.48\textwidth}


\begin{document}

\noindent
\vspace*{1ex}
\begin{minipage}[t]{.45\linewidth}
\strut\vspace*{-\baselineskip}\newline
\includegraphics[height=.9cm]{./figs/Inf-Logo_black_en-eps-converted-to.pdf}
\includegraphics[height=.9cm]{./figs/par-logo}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\linewidth}
\flushright{
Research Group for Parallel Computing\\%
Faculty of Informatics\\%
TU Wien}
\end{minipage}
\vspace*{1ex}

\hrule 

\vspace*{2ex}

\begin{center}
{\LARGE\textbf{Parallel Computing}}\\
{\large{}%
  2022S\\
  Ãœbungsblatt 1\\
}
\end{center}

\hrule 
\vspace*{1ex}

\noindent
1: Jakob, Roithinger, 52009269\\
2: Elias, Pinter, 12023962\\
3: Kurdo-Jaroslav, Asinger, 01300351

\vspace*{1ex}
\hrule 

\exercise{1}

\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void mv(int m, int n, double M[m][n], double V[n], double W[m])
{
  int i, j;

  for (i=0; i<m; i++) {
    W[i] = 0.0;
    for (j=0; j<n; j++) {
      W[i] += M[i][j]*V[j];
    }
  }
}
\end{lstlisting}
\end{minipage}

\answer{1}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void mv(int m, int n, double M[m][n], double V[n], double W[m]){
  par(0<=i<m){
  	// O(n) parallel time steps
    W[i] = 0.0;
    for(int j = 0; j < n; j++){
      // summing into each cell in result vector 
      W[i] += M[i][j] * V[j];
    }
  }
}
\end{lstlisting}
\end{minipage}
Requires the CREW PRAM model (concurrent read on V - no concurrent writes).

\answer{2}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void mv(int m, int n, double M[m][n], double V[n], double W[m]) {
  
  par(0<=i<m, 0<=j<n){
    // doing actual multiplication in O(1) time steps (requires CREW PRAM)
    M[i][j] *= V[j];
  }

  // sum rows of Matrix in O(log(n)) time steps
  par(0<=i<m){
    int offset = 1;
    int betweenGap = 2;
    // sum of Matrix[i] in O(log(n)) time steps
    while(offset < n){
      // EREW summing, actually only n/betweenGap processors needed
      par(0<=j<n) {
        if(offset + j * betweenGap < n){
          M[i][j * betweenGap] += M[i][offset + j * betweenGap];
        }
      }
      offset *= 2;
      betweenGap *= 2;
    }
  }
  
  par(0<=i<n){
    // copying into result vector in O(1) time steps
    W[i] = M[i][0];
  }
}
\end{lstlisting}
\end{minipage}
Requires the CREW PRAM model (concurrent read on V - no concurrent writes). \\
The matrix-vector multiplication requires $O(n \cdot m)$ work. Summation of matrix rows requires $O(n \cdot m)$ work. Copying the result back into the result vector takes $O(n)$ work. This is work optimal, as matrix-vector multiplication requires  $O(n \cdot m)$ work for the best sequential algorithm.

\answer{3}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void mv(int m, int n, double M[m][n], double V[n], double W[m]){
  double C[m][n];
  par(0<=j<n){
    // copying vector into intermediate matrix in O(1) time steps
    C[0][j] = V[j];
  }

  // making m copies of V in O(log(m)) time steps
  offset = 1;
  // O(log(n)) loops
  while(offset < m){
    // duplicating existing vectors in O(1)
    // offset, m private for each thread i.e in OpenMP firstprivate(offset, m)
    par(0<=i<m){
      if(i + offset < m){
        par(0<=j<n){
          C[i + offset][j] = C[i][j]
        }
      }
    }
    offset *= 2;
  }

  par(0<=i<m, 0<=j<n){
    // calculating summands in O(1) time steps
    M[i][j] = M[i][j] * C[i][j];
  }
    
  par(0<=i<m){
    int offset = 1;
    int betweenGap = 2;
    // sum row [i] of Matrix in O(log(n)) time steps
    while(offset < n){
      // EREW summing, actually only n/betweenGap processors needed
      // offset, n private for each thread i.e in OpenMP firstprivate(offset, n)
      par(0<=j<n) {
        if(offset + j * betweenGap < n){
          M[i][j * betweenGap] += M[i][offset + j * betweenGap];
        }
      }
      offset *= 2;
      betweenGap *= 2;
    }
  }

  par(0<=i<n){
    // copying into result vector in O(1) time steps
    W[i] = M[i][0];
  }
}
\end{lstlisting}
\end{minipage}
Requires the EREW PRAM model (no concurrent reads or writes). \\
The essential modification (compared to the algorithm from 1.2) is to make copies of the vector before multiplying. This obviously requires more operations, however the work done is still in  $O(n \cdot m)$.

\exercise{2}

\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
for (i=0; i<n; i++) {
  int count = 0;
  for (j=0; j<i; j++) {
    if (a[j]<=a[i]) count++;
  }
  j++;
  for (; j<n; j++) {
    if (a[j]<a[i]) count++;
  }
  b[count] = a[i];
}
for (i=0; i<n; i++) a[i] = b[i];
\end{lstlisting}
\end{minipage}

\answer{1}
The program sorts a given array of integers in ascending order. A comparison with "$a[j] \leq \ a[i]$" has to be done in for all $j$ where $j \leq i$, so that the program also sorts arrays with duplicates. If $\leq$ is not used for all$j \leq i$, values written into b (and subsequently back into a) will be written to the same index for the small values, causing ``gaps'' (some array entries in b (therefore also in a at the end) will have undefined/default initialization values). Using $\leq$ also guarantees stability, meaning that the relative order of equal elements in the input will be sustained in the output.

\answer{2}
The asymptotic sequential work of the given algorithm is $O(n^{2})$. The outer loop is executed n times. Within this loop, there are two nested loops which have a total of $n-1 (= O(n))$ iterations (j run from 0 to i, and then from i + 1 to n), each with $O(1)$ work. After the first (outer) loop, there is another loop which is executed n times, with $O(1)$ work per iteration. This concludes to the work of $O(n^{2})$. 
% superfluous
% Here is the code once again, with explanation regarding work in the comments:


%\begin{minipage}[t]{1.0\linewidth}
%\begin{lstlisting}
%for (i=0; i<n; i++) {            // canonical loop dependent on n
%  int count = 0;                 // constant work
%  for (j=0; j<i; j++) {          // linear loop dependent on i
%    if (a[j]<=a[i]) count++;     // constant work
%  }
%  j++;                           // constant work
%  for (; j<n; j++) {             // linear loop dependent on the i-loop and n
%    if (a[j]<a[i]) count++;      // constant work
%  }
%  b[count] = a[i];               // constant work
%}
%for (i=0; i<n; i++)              // canonical loop dependent on n
%  a[i] = b[i];                   // constant work
%\end{lstlisting}
%\end{minipage}
\noindent
Exact calculation:
$ n \cdot (1 + i \cdot 1 + 1 + (n-i) \cdot 1 + 1) + n \cdot 1 = n \cdot (n + 3) + n = n^2 + 4n $\\
$n^2 + 4n \in O(n^2)$ \\
This is a very simplified calculation. The j-loop's workload rises as the i-loop's workload shrinks. Generally, within one outer-loop iteration, the sum of the iterations of the first and second j-loop together is n. However, it does not make in difference for the big O notation.

\answer{3}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
#include <omp.h>
// assume a, b initialized arrays, each with a capacity of n integers
#pragma omp parallel default(none) shared(a, b) firstprivate(n)
{
  int i;
  #pragma omp for
  for (i = 0; i < n; i++) {
   int count = 0;
   int j;
   for (j = 0; j < i; j++) {
    if (a[j] <= a[i]) count++;
   }
    j++;
   for (; j < n; j++) {
    if (a[j] < a[i]) count++;
   }
   b[count] = a[i];
  }
  // implicit barrier
  #pragma omp for
  for (i = 0; i < n; i++) {
   a[i] = b[i];
 }
}
\end{lstlisting}
\end{minipage}

\answer{4}
The outer loop is embarrassingly parallel, because the iterations are independent of each other. Therefore, the speedup is linear (compared to the sequential execution of this algorithm). So assuming $n \geq p$ the runtime will be in:\\
\begin{math}O(\frac{n^2}{p})\end{math}\\

\answer{5}
Relative speedup will be linear as long as $p \leq n$.\\

\begin{math}SRel_{p}(n)=\frac{T_{par}(1,n)}{T_{par}(p,n)} = \frac{n^2}{n^2/p} = p \end{math}\\
The absolute speedup has to be compared with the best known sequantial algorithm for this problem (sorting). Mergesort is on of these, running in \begin{math}O(n \cdot log(n))\end{math}. The absulote speed-up is not linear.\\

\begin{math}S_{p}(n)=\frac{T_{seq}(n)}{T_{par}(p,n)} = \frac{n \cdot log(n)}{n^2/p} = \frac{log(n) \cdot p}{n} \end{math}\\



\exercise{3}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
# pragma omp parallel for schedule ( runtime )
for (i = 0; i < n ; i++) {
  a[i] = omp_get_thread_num();
  t[omp_get_thread_num()]++;
}
\end{lstlisting}
\end{minipage}
\answer{1}
a[i] tracks which thread executed iteration \emph{i} in the for loop.
t[i] tracks how many iterations were executed by thread \emph{i}.

\answer{2}


\begin{tabular}{rrrrrrrrrrrrrrrrrrrr}
  \toprule
 case /  $i$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18  \\
  \midrule
\texttt{static}     & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 & 3 & 4 & 4 & 4 & 5 & 5 & 5 \\
\texttt{static,2}   & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 & 5 & 0 & 0 & 1 & 1 & 2 & 2 & 3 \\
\texttt{static,5}   & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 2 & 3 & 3 & 3 & 3 \\
\texttt{static,6}   & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 2 & 2 & 3 \\ 
\texttt{dynamic,1}  & 0 & 2 & 5 & 0 & 0 & 0 & 2 & 1 & 5 & 4 & 2 & 3 & 5 & 1 & 0 & 2 & 4 & 5 & 3 \\
\texttt{dynamic,2}  & 0 & 0 & 4 & 4 & 1 & 1 & 3 & 3 & 5 & 5 & 2 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  \texttt{guided,3} & 0 & 0 & 0 & 0 & 4 & 4 & 4 & 1 & 1 & 1 & 0 & 0 & 0 & 5 & 5 & 5 & 4 & 4 & 4 \\
  \bottomrule
\end{tabular}\\

\begin{tabular}{rrrrrrr}
  \toprule
case / $t$             & $t[0]$ & $t[1]$ & $t[2]$ & $t[3]$ & $t[4]$ & $t[5]$ \\  
  \midrule
\texttt{static}    & 4 & 3 & 3 & 3 & 3 & 3 \\
\texttt{static,2}  & 4 & 4 & 4 & 3 & 2 & 2 \\
\texttt{static,5}  & 5 & 5 & 5 & 4 & 0 & 0 \\
\texttt{static,6}  & 6 & 6 & 6 & 1 & 0 & 0 \\
\texttt{dynamic,1} & 5 & 2 & 4 & 2 & 2 & 4 \\
\texttt{dynamic,2} & 9 & 2 & 2 & 2 & 2 & 2 \\
\texttt{guided,3}  & 7 & 3 & 0 & 0 & 6 & 3 \\
    \bottomrule
\end{tabular}

\answer{3}
False sharing is a possible performance issue:
Multiple threads (on different cores) may want to access t (at different indexes i and j) at close points in time. If two indexes i, j are close enough and positioned in a way that t[i] and t[j] are on the same cache line, and t[i] t[j] are accessed by different threads, they will be ``falsely shared'' and cache coherency activity will occur, which impacts performance considerably. 


\exercise{4}

\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
i = 0; j = 0; k = 0; 
while (i<n&&j<m) {
   C[k++] = (A[i]<=B[j]) ? A[i++] : B[j++]; 
}
while (i<n) C[k++] = A[i++]; 
while (j<m) C[k++] = B[j++];
\end{lstlisting}
\end{minipage}

\answer{1}
Assuming i, j, k are in registers. \\
Example for a best case input for A and B:
A = [0,0,0,0 ...] 
B = [1,1,1,1 ...] \\
This way A[0] will be fetched with a cache miss, then B[0] will be fetched with a cache miss (stored in a register afterwards), overwriting the cache line with A[0] stored. Then A[0] will be inserted at C[0].
Then A[1] will be fetched, resulting in a cache miss, however the rest of A will be fetched and inserted into C, resulting in a cache miss every 16 elements. \\
Then all of B will be inserted into C, also resulting in a cache miss every 16 elements. \\
Total cache misses: 2 + 2n/16 = 2 + n/8, \textbf{cache miss rate:} 1/16

\answer{2}
Example for a worst case input for A and B:
A = [1,2,3,4 ...] 
B = [1,2,3,4 ...] \\
If A and B contain the same values that are also strictly increasing, alternating at every iteration first A[i] <= B[j] will hold, and in the next A[i] > B[j]. At every iteration a new element has to be fetched from memory, but a miss will always occur, as the cache line will contain elements from A[i] when an element from B[j] is needed, and vice versa.  
Total cache misses: 2n, \textbf{cache miss rate:} 1.0

\exercise{5}

\answer{1}
A parallel section is opened, with to omp for constructs. In the first for-construct every element in \emph{A} is ranked in the array \emph{B} and inserted accordingly in \emph{C}. It is marked as no wait, as there are no data conflicts. In the second for-construct every element in \emph{B} is ranked in the array \emph{A} and then inserted accordingly in \emph{C}. Ranking is done sequentially by binary search in O(log(n)), inserting into \emph{C} is done for n + m elements in parallel, in O(1) time. Therefore the runtime is $ O\left(\frac{n}{p} \cdot \log(m) \right) + O\left(\frac{m}{p} \cdot \log(n)\right) = 
O\left(\frac{n \cdot \log(m) + m \cdot \log(n)}{p}\right)$.
\\
\\
Code can be found in \textit{merge1.c}


\answer{2}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Rank-each-element_1000000_2000000}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Rank-each-element_10000000_20000000} \\
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Rank-each-element_100000000_200000000} \\

% TODO review "discussion"
\noindent
Looking at the benchmark plots, a very definitive linear absolute speed-up can be recognized for all 3 input sizes. However even for 32 processors, it only lies around 0.6 (parallelization does not make sense for absolute speed up < 1). The graphs for the different input sizes look identical, only the y-axis scales for average times differ by factors of 10, which makes sense as the input sizes differ by factors of 10 and there should barely be any idle time as there is \textit{more than enough} work for each processor.

\answer{3}
To make the merge stable, nothing has to be done to preserve relative order of equal elements in A as well as B in C.
To make sure equal elements of A come before equal elements of B: \\
Rewrite rank algorithm:\\
rank{\_}for{\_}A(a, B, m) returns a j such that $B[j] < a \leq B[j + 1]$ \\
and another algorithm: \\
rank{\_}for{\_}B(b, A, n) returns an i such that $A[i] \leq b < A[i + 1]$

\newpage
\exercise{6}
\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void merge_corank(double A[], int n, double B[], int m, double C[])
{
  int t; // number of blocks (threads)
  int i;
  
  int coj[t+1];
  int cok[t+1];
  
  for (i=0; i<t; i++) {
    corank(i*(n+m)/t,A,n,&coj[i],B,m,&cok[i]);
  }
  coj[t] = n;
  cok[t] = m;
  
  for (i=0; i<t; i++) {
    merge(&A[coj[i]],coj[i+1]-coj[i],
    &B[cok[i]],cok[i+1]-cok[i],
    &C[i*(n+m)/t]);
  }
}
\end{lstlisting}
\end{minipage}

\answer{1}
The asymptotic complexity can be calculated as follows:\\
\\
$O\left(t \cdot \log(n+m) + t \cdot \frac{n+m}{t} \right) = O(n + m + t \cdot \log(n+m))$
\\
\\
To compare this to the sequential complexity $t=1$ should be chosen (this way work done is also in $O(n+m)$).

\answer{2}
Code can be found in \textit{merge2.c}
\answer{3}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Co-rank_1000000_2000000}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Co-rank_10000000_20000000}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Co-rank_100000000_200000000}
\\
\\
This parallel merging algorithm performs a lot better than the last, with absolute speed-ups ranging between 10-20 for 32 processors. Even though n+m >> p, the absolute speed-up line is not linear (algorithm is not strongly scaling), which makes sense, as the amount of processors corresponds to t in the runtime-complexity formula ($O(n + m + t \cdot \log(n+m))$).

\answer{4}
Currently in the program there is an implicit barrier (synchronization) required after calculating the co-ranks, as each thread needs the co-ranks at index i and i + 1 (i=omp\_get\_thread\_num):\\
  \begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
  #pragma omp for SCHEDULE_STRATEGY
  for (i = 0; i < t; i++) {
      corank(i * (n + m) / t, A, n, &coj[i], B, m, &cok[i]);
  }
  \end{lstlisting}
\end{minipage}
If we calculate the co-ranks twice per block as follows we can get rid of this synchronization:
  \begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
  if(omp_get_thread_num != t-1){
    corank(omp_get_thread_num() * (n + m) / t, A, n, &coj[i], B, m, &cok[i]);
    corank((omp_get_thread_num() + 1) * (n + m) / t, A, n, &coj[i], B, m, 
    &cok[i]);
  }
  seq_merge1(&A[coj[i]], coj[omp_get_thread_num + 1] - coj[omp_get_thread_num], 
  &B[cok[i]], cok[omp_get_thread_num + 1] - cok[omp_get_thread_num], 
  &C[omp_get_thread_num * (n + m) / t]);
  \end{lstlisting}
\end{minipage}
This way there are around twice as many co-rank operations performed, however, no synchronization is necessary.

\exercise{7}

\begin{minipage}[t]{1.0\linewidth}
\begin{lstlisting}
void merge_divconq(double A[], int n, double B[], int m, double C[])
{
  int i;

  if (n==0) { // task parallelize for large n
    for (i=0; i<m; i++) C[i] = B[i];
  } else if (m==0) { // task parallelize for large m
    for (i=0; i<n; i++) C[i] = A[i];
  } else if (n+m<CUTOFF) {
    merge(A,n,B,m,C); // sequential merge for small problems
  } else {
    int r = n/2;
    int s = rank(A[r],B,m);
    C[r+s] = A[r];
    merge_divconq(A,r,B,s,C);
    merge_divconq(&A[r+1],n-r-1,&B[s],m-s,&C[r+s+1]);
  }
}
\end{lstlisting}
\end{minipage}

\answer{1}
Code can be found in \textit{merge3.c}
\answer{2}
The CUTOFF should not be too small, to minimize the recursion overhead. It should also be chosen small enough, that processors do not have high idle times. To achieve this the CUTOFF should be directly proportional to $m+n$ and indirectly proportional to $p$. Testing empirically showed that CUTOFF=$ x\cdot \frac{n \cdot m}{t}$ for x < 5 (where n and m are the lengths of the arrays A and B before performing any recursion steps) had the fastest runtimes.


\answer{3}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Divide-and-Conquer_1000000_2000000}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Divide-and-Conquer_10000000_20000000}
\includegraphics[scale=0.4,page=1]{../plots/merge_plot_Divide-and-Conquer_100000000_200000000}
\\
\\
In the first plot we can see that the speed up monotonically increases up to using 16 processors, but then dips for 32 processors, which is likely due to the extra overhead that comes with more processors. On the other two plots with larger problem instances this behavior is not observed for $p \leq 32$.
\exercise{8}

\answer{1}
Code can be found in \textit{mv1.c}

\answer{2}
%not asked
%We would expect the algorithm to have a linear relaitive speedup, due to a good load balance on the processes - as long as $p \leq m$  (rows of the matrix $A$). Generating the sub-input is manageable in $O\left(\frac{m \cdot n}{p}\right)$ if the contents have to be read and re-written. If it is just generated by using pointer-arithmetics, then  it can be even done in $O\left(1\right)$.
The complexity of the multiplication one process has to calculate is in $O(\frac{m \cdot n}{p})$. The complexities of all MPI-functions must be added. MPI\_Allgather and MPI\_Exscan run in $O(n + \log(p))$. As for MPI\_Allgatherv, we assume that it also has the complexity of $O(n + \log(p))$. In the call of Allgatherv the input size is dependent on $n$ (number of columns in matrix $A$), because we gather the vector $x$ which has $n$ elements. Therefore $(n + log(p))$ has to be added. As for the other calls, the input size is dependent on $p$, so $(p + log(p))$ has to be added as well. After simplification, the complexity should be $O\left(\frac{m \cdot n}{p} + n + p\right)$.\\

\noindent The best known sequential algorithm vor matrix-vector multiplication runs in $O(m \cdot n)$. However, if we do not use more processes than input lines $m$, both $n$ and $p$ will be dominated by $\frac{m \cdot n}{p}$.
\\[0.5 em]
$S_{p}(n)=\frac{T_{seq}(n)}{T_{par}(p,n)} = \frac{m \cdot n}{(m \cdot n)/p } = p$ (if $p \leq\ m$)
\\[0.5 em]
In worst case, the speedup is not linear anymore:
\\[0.5 em]
$\displaystyle S_{p}(n)=\frac{T_{seq}(n)}{T_{par}(p,n)} = \frac{m \cdot n}{n \cdot m/p + n + p} = \frac{m \cdot n \cdot p}{m \cdot n + p \cdot  n + p^2} = \frac{p}{1 + p/m + p^2/(m \cdot n)} = \frac{m \cdot p}{m + p + p^2/n}$
\\[0.5 em]
In not too many processes are used (as stated above), then the parallel efficiency would be 1:
\\[0.5 em]
$\displaystyle E_{p}(n)=\frac{T_{seq}(n)}{pT_{par}(p,n)} = \frac{m \cdot n}{p \cdot (m \cdot n)/p} = 1$
\\[0.5 em]
If too many $p$ will be used, the errifiency will turn into:
\\[0.5 em]
$\displaystyle E_{p}(n)=\frac{T_{seq}(n)}{pT_{par}(p,n)} = \frac{1}{1 + p/m + p^2/(m \cdot n)}$
\\[0.5 em]
As we can see, the efficiency will drop further the more we increase $p$.


\answer{3}
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Row-wise-distribution_300} 
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Row-wise-distribution_600} \\
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Row-wise-distribution_1200} 
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Row-wise-distribution_12000} \\

\noindent For smaller input sizes ($\leq$1200) the average time increases with increased processor count. Respectively the absolute speed up decreases. This can be explained by the fact that the communication required to distribute the vector dominates the time saved by parallelizing the matrix-vector multiplication. For example for n=300, p=256 each processor only receives 1-2 rows, and so only has to do 2 * n (=600) multiplications and additions. For n=12,000 the absolute speed-up is monotonically increasing with increasing processors (at least until p=256). Here each processor has to do around 45 * n (=540,000) multiplications and additions.

\exercise{9}

\answer{1}
Code can be found in \textit{mv2.c}
\answer{2}
We would expect the algorithm to have a linear relaitive speedup, due to a good load balance on the processes - as long as $p \leq\ n$  (columns of the matrix $A$). 
%Generating the sub-input is manageable in $O\left(\frac{m \cdot n}{p}\right)$ if the contents have to be read and re-written.
If it is just generated by using pointer-arithmetics, then  it can be even done in $O\left(1\right)$. The complexity of the multiplication one process has to calculate is in $\frac{m \cdot n}{p}$. The complexities of all MPI-functions must be added. MPI\_Allgather,  MPI\_Reduce\_scatter run in $O(n + \log(p))$. As for Reduce\_scatter, the input size is dependent on $m$ (number of rows in matrix $A$), because the reduction operation is performed on the vector $b$ which has $m$ elements. Therefore $(m + \log(p))$ has to be added. As for Allgather, the input size is dependent on $p$, so $(p + \log(p))$ has to be added as well. After simplification, the complexity should be $O\left(\frac{m \cdot n}{p} + m + p\right)$.\\

\noindent The best known sequential algorithm for matrix-vector multiplication runs in $O(m \cdot n)$. However, if we do not use more processes than input columns $n$, both $m$ and $p$ will be dominated by $\frac{m \cdot n}{p}$.\\
$\displaystyle S_{p}(n)=\frac{T_{seq}(n)}{T_{par}(p,n)} = \frac{m \cdot n}{(m \cdot n)/p } = p$ (if $p \leq\ n$)
\\[0.5 em]
In worst case, the speedup is not linear anymore:
\\[0.5 em]
$\displaystyle S_{p}(n)=\frac{T_{seq}(n)}{T_{par}(p,n)} = \frac{m \cdot n}{m \cdot n/p + m + p} = \frac{m \cdot n \cdot p}{m \cdot n + p \cdot  m + p^2} = \frac{p}{1 + p/n + p^2/(m \cdot n)} = \frac{n \cdot p}{n + p + p^2/m}$
\\[0.5 em]
In not too many processes are used (as stated above), then the parallel efficiency would be 1:
\\[0.5 em]
$\displaystyle E_{p}(n)=\frac{T_{seq}(n)}{pT_{par}(p,n)} = \frac{m \cdot n}{p \cdot (m \cdot n)/p} = 1 $
\\[0.5 em]
If too many $p$ will be used, the errifiency will turn into:
\\[0.5 em]
$\displaystyle E_{p}(n)=\frac{T_{seq}(n)}{pT_{par}(p,n)} = \frac{1}{1 + p/n + p^2/(m \cdot n)} $
\\[0.5 em]
As we can see, the efficiency will drop further the more we increase $p$.

\answer{3}
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Column-wise-distribution_300} 
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Column-wise-distribution_600} \\
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Column-wise-distribution_1200} 
\includegraphics[scale=0.4,page=1]{../plots/matrix_vector_plot_Column-wise-distribution_12000} \\

\noindent Similar as with row-wise distribution, for smaller input sizes ($\leq$600) the average time increases with increased processor count. Respectively the absolute speed up decreases. For n=1200 p=128, there seems to be just enough work for each processor for the communication overhead to be worth it, however with p=256, the calculation time increases. For n=12,000 the absolute speed-up is monotonically increasing with increasing processors (at least until p=256). Comparing for n=12,000 column wise distribution has a higher absolute speed up than row-wise, which can be explained by the fact that the column wise algorithm uses fewer communication rounds.

\end{document}
